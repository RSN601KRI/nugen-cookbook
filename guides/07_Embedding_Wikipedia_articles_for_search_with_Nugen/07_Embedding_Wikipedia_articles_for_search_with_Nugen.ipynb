{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XVMs4Y399o1-"
   },
   "source": [
    "## **Nugen Intelligence**\n",
    "<img src=\"https://nugen.in/logo.png\" alt=\"Nugen Logo\" width=\"200\"/>\n",
    "\n",
    "Domain-aligned foundational models at industry leading speeds and zero-data retention! To learn more, visit [Nugen](https://docs.nugen.in/introduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eMYwmArPzWby"
   },
   "source": [
    "### **Generating Embeddings with Nugen API**\n",
    "\n",
    "This lesson demonstrates how to generate embeddings for texts using Nugen embeddings APIs. To do that, we will be following the steps mentioned below:\n",
    "1. Extract information from Wikipedia\n",
    "2. Break it into smaller sections\n",
    "3. Generate high-performance embeddings using the [Nugen API](https://docs.nugen.in/introduction)\n",
    "\n",
    "\n",
    "With Nugen’s cutting-edge API, you can easily generate embeddings that are optimized for speed and accuracy, enabling faster and more relevant results in your applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Wikipedia Articles for Semantic Search (with Nugen)\n",
    "\n",
    "### Purpose\n",
    "This notebook demonstrates how to:\n",
    "- Fetch Wikipedia articles programmatically\n",
    "- Convert article text into vector embeddings using Nugen\n",
    "- Perform semantic search over Wikipedia content\n",
    "\n",
    "### Why this matters\n",
    "Traditional keyword search fails to capture meaning.\n",
    "Embedding-based search enables:\n",
    "- Context-aware retrieval\n",
    "- Better relevance\n",
    "- Scalable knowledge search systems\n",
    "\n",
    "### Prerequisites\n",
    "- Python 3.9+\n",
    "- Valid Nugen API credentials\n",
    "- Internet access\n",
    "- Required Python libraries installed\n",
    "\n",
    "### How to run\n",
    "Run each cell from top to bottom.\n",
    "If an error occurs, check the **Troubleshooting** section at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FarRQSZFzi_Y"
   },
   "source": [
    "### **Setup**\n",
    "**Install Required Libraries**\n",
    "We'll install the required Python libraries to interact with Wikipedia, split sections, and count tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: It is recommended to use a virtual environment before installing dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: C:\\Users\\Roshni\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install --quiet mwclient==0.11.0 mwparserfromhell==0.6.6 pandas==1.5.3 tiktoken==0.7.0 openai==1.34.0 requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2bhcoVBn1B8W"
   },
   "source": [
    "**Import Necessary Libraries**\n",
    "\n",
    "These libraries help us work with Wikipedia articles, clean and process them, and prepare them for embedding using the Nugen API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "kk-ug8DD1N7E"
   },
   "outputs": [],
   "source": [
    "# MediaWiki client for interacting with Wikipedia\n",
    "import mwclient\n",
    "\n",
    "# Parser to clean and extract Wikipedia article content\n",
    "import mwparserfromhell\n",
    "\n",
    "# Data handling and tabular operations\n",
    "import pandas as pd\n",
    "\n",
    "# Standard utilities\n",
    "import re\n",
    "import random\n",
    "import requests\n",
    "\n",
    "# Token counting for embedding size control\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n1sn9yDQ1QHw"
   },
   "source": [
    "### **Access the Nugen API**\n",
    "\n",
    "**API Key Setup**\n",
    "\n",
    "First, we need to set up the Nugen API to generate embeddings. To do this, you'll need an API key from Nugen. To access free API keys, you can visit [Nugen Dashboard](https://nugen-platform-frontend.azurewebsites.net/dashboard) Once you have your API key, make sure to replace <your_api_key> in the code with the actual key you get from Nugen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Secure API Key Setup\n",
    "\n",
    "For security reasons, API keys should **not be hardcoded**.\n",
    "Set your Nugen API key as an environment variable before running this notebook.\n",
    "\n",
    "**Example (Linux / macOS):**\n",
    "```bash\n",
    "export NUGEN_API_KEY=\"your_api_key_here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AjFprmJc1YSV",
    "outputId": "777388ec-be79-4772-d6d1-2b22106eaa6a"
   },
   "outputs": [],
   "source": [
    "url_api_server = \"https://api.nugen.in/inference/embeddings\"\n",
    "api_key = \"nugen-LCKMzU2MmCZiSXCcOOm3Ow\"\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {api_key}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pjela3Yg15H1"
   },
   "source": [
    "### **Get Wikipedia Articles**\n",
    "**Choosing Wikipedia Articles**\n",
    "\n",
    "We are going to retrieve articles related to the 2022 Winter Olympics using a Wikipedia category. This section searches for all pages within that category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k-fOFKF6162k",
    "outputId": "c3bb815c-07be-484c-ed9d-358f27502790"
   },
   "outputs": [],
   "source": [
    "CATEGORY_TITLE = \"Category:2022 Winter Olympics\"\n",
    "WIKI_SITE = \"en.wikipedia.org\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yvNIQ3Ut2QLR"
   },
   "source": [
    "**Extract Article Titles**\n",
    "\n",
    "We now gather all the article titles under this category.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xjnqw5mR2DIW",
    "outputId": "60fcb86f-6cde-4a6e-96ca-ccf42a143261"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 34 article titles for processing.\n"
     ]
    }
   ],
   "source": [
    "def titles_from_category(category, max_depth):\n",
    "    \"\"\"Return a set of page titles in a given Wiki category and its subcategories.\"\"\"\n",
    "    titles = set()\n",
    "    for cm in category.members():\n",
    "        if type(cm) == mwclient.page.Page:\n",
    "            titles.add(cm.name)\n",
    "        elif isinstance(cm, mwclient.listing.Category) and max_depth > 0:\n",
    "            deeper_titles = titles_from_category(cm, max_depth=max_depth - 1)\n",
    "            titles.update(deeper_titles)\n",
    "    return titles\n",
    "\n",
    "# Initialize the Wikipedia client\n",
    "site = mwclient.Site(WIKI_SITE)\n",
    "category_page = site.pages[CATEGORY_TITLE]\n",
    "titles = titles_from_category(category_page, max_depth=1)\n",
    "\n",
    "# Select 20% of the articles for processing you can modify this according to your use case.\n",
    "sample_size = int(0.2 * len(titles))\n",
    "sampled_titles = random.sample(list(titles), sample_size)\n",
    "print(f\"Selected {len(sampled_titles)} article titles for processing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zbTskCOa5Cy_"
   },
   "source": [
    "**How It Works**\n",
    "\n",
    "1. **`titles_from_category` function**: Recursively traverses a Wikipedia category and collects article titles from the category and its subcategories, up to a configurable depth.\n",
    "2. **`max_depth` parameter**: Limits recursion depth to prevent excessive API calls and infinite category loops."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y78UsWuzACca"
   },
   "source": [
    "### **Chunk Documents**\n",
    "Now that we have our reference documents, we need to prepare them for search.\n",
    "\n",
    "For this specific example on Wikipedia articles, we'll:\n",
    "\n",
    "* Discard less relevant-looking sections like External Links and Footnotes\n",
    "* Clean up the text by removing reference tags (e.g., ), whitespace, and super short sections\n",
    "* Split each article into sections\n",
    "* Prepend titles and subtitles to each section's text to preserve semantic context for embedding models\n",
    "* If a section is long (say, > 1,600 tokens), we'll recursively split it into smaller sections, trying to split along     semantic boundaries like paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EQ3jbtSw61eO",
    "outputId": "33bc17d3-861c-4ddc-fd9f-6ee6fe4c375b"
   },
   "outputs": [],
   "source": [
    "SECTIONS_TO_IGNORE = [\n",
    "    \"See also\",\n",
    "    \"References\",\n",
    "    \"External links\",\n",
    "    \"Further reading\",\n",
    "    \"Footnotes\",\n",
    "    \"Bibliography\",\n",
    "    \"Sources\",\n",
    "    \"Citations\",\n",
    "    \"Literature\",\n",
    "    \"Footnotes\",\n",
    "    \"Notes and references\",\n",
    "    \"Photo gallery\",\n",
    "    \"Works cited\",\n",
    "    \"Photos\",\n",
    "    \"Gallery\",\n",
    "    \"Notes\",\n",
    "    \"References and sources\",\n",
    "    \"References and notes\",\n",
    "]\n",
    "\n",
    "def all_subsections_from_section(section, parent_titles, sections_to_ignore):\n",
    "    \"\"\"Extract subsections from a Wikipedia section.\"\"\"\n",
    "    headings = [str(h) for h in section.filter_headings()]\n",
    "    title = headings[0]\n",
    "    if title.strip(\"=\" + \" \") in sections_to_ignore:\n",
    "        return []\n",
    "    titles = parent_titles + [title]\n",
    "    full_text = str(section)\n",
    "    section_text = full_text.split(title)[1]\n",
    "    if len(headings) == 1:\n",
    "        return [(titles, section_text)]\n",
    "    else:\n",
    "        first_subtitle = headings[1]\n",
    "        section_text = section_text.split(first_subtitle)[0]\n",
    "        results = [(titles, section_text)]\n",
    "        for subsection in section.get_sections(levels=[len(titles) + 1]):\n",
    "            results.extend(all_subsections_from_section(subsection, titles, sections_to_ignore))\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The all_subsections_from_section function is designed to extract subsections from a specific section of a Wikipedia article. This function is used in the context of processing a page’s text, finding headings, and breaking the content down into smaller chunks (subsections). It helps you organize the text under each heading while ignoring certain sections you don't want to include (like references or external links).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wYv5v4X82gEl",
    "outputId": "b0aa84bf-18e7-4166-98ee-7ddcb5495eb1"
   },
   "outputs": [],
   "source": [
    "def all_subsections_from_title(\n",
    "    title: str,\n",
    "    sections_to_ignore: set[str] = SECTIONS_TO_IGNORE,\n",
    "    site_name: str = WIKI_SITE,\n",
    ") -> list[tuple[list[str], str]]:\n",
    "    \"\"\"From a Wikipedia page title, return a flattened list of all nested subsections.\n",
    "    Each subsection is a tuple, where:\n",
    "        - the first element is a list of parent subtitles, starting with the page title\n",
    "        - the second element is the text of the subsection (but not any children)\n",
    "    \"\"\"\n",
    "    site = mwclient.Site(site_name)\n",
    "    page = site.pages[title]\n",
    "    text = page.text()\n",
    "    parsed_text = mwparserfromhell.parse(text)\n",
    "    headings = [str(h) for h in parsed_text.filter_headings()]\n",
    "    if headings:\n",
    "        summary_text = str(parsed_text).split(headings[0])[0]\n",
    "    else:\n",
    "        summary_text = str(parsed_text)\n",
    "    results = [([title], summary_text)]\n",
    "    for subsection in parsed_text.get_sections(levels=[2]):\n",
    "        results.extend(all_subsections_from_section(subsection, [title], sections_to_ignore))\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Subsections from a Wikipedia Article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function takes a Wikipedia page title and returns all the subsections of that page, along with their corresponding parent titles.\n",
    "It extracts the page's text, identifies headings, and organizes the content into a structured list of tuples.\n",
    "\n",
    "Each tuple contains:\n",
    "1. A list of parent titles (starting with the page title)\n",
    "2. The text of the subsection (excluding any sub-subsections)\n",
    "\n",
    "### Function Parameters\n",
    "\n",
    "1. **title: str**  \n",
    "   The title of the Wikipedia page to extract subsections from  \n",
    "   *(e.g., `\"Python (programming language)\"`)*\n",
    "\n",
    "2. **sections_to_ignore: set[str]** *(default: `SECTIONS_TO_IGNORE`)*  \n",
    "   A set of section titles that should be excluded from extraction, such as  \n",
    "   `\"References\"` or `\"External links\"`\n",
    "\n",
    "3. **site_name: str** *(default: `WIKI_SITE`)*  \n",
    "   The Wikipedia domain to use  \n",
    "   *(e.g., `\"en.wikipedia.org\"` for English Wikipedia)*\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "Large Wikipedia articles are not ideal for direct embedding due to their size and mixed topics.\n",
    "By splitting articles into logically grouped subsections while preserving hierarchical context, this approach:\n",
    "- Improves embedding quality\n",
    "- Enhances semantic search relevance\n",
    "- Reduces noise from non-informative sections\n",
    "\n",
    "This structured extraction enables efficient document chunking and improves semantic retrieval accuracy in downstream embedding and RAG workflows.\n",
    "\n",
    "### Clean Up Sections\n",
    "\n",
    "After extracting the subsections, we clean the text to remove non-content artifacts such as:\n",
    "- Reference markers (e.g., `[1]`, `[2]`)\n",
    "- Extra whitespace\n",
    "- Formatting noise introduced during wiki parsing\n",
    "\n",
    "This ensures that each text chunk is embedding-ready and does not introduce irrelevant tokens into the vector store."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RIGilLid2i5l"
   },
   "source": [
    "The function above splits the articles into smaller sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "83hedxcv2jvk",
    "outputId": "45c80699-f4f1-4a87-a64d-93afa3c55003"
   },
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "HTTPSConnectionPool(host='en.wikipedia.org', port=443): Max retries exceeded with url: /w/api.php?meta=siteinfo%7Cuserinfo%7Cuserinfo&siprop=general%7Cnamespaces&uiprop=groups%7Crights%7Cblockinfo%7Chasmsg&continue=&action=query&format=json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x0000018F71DEBA30>: Failed to establish a new connection: [WinError 10053] An established connection was aborted by the software in your host machine'))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConnectionAbortedError\u001b[0m                    Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\urllib3\\connection.py:198\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 198\u001b[0m     sock \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43msocket_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\urllib3\\util\\connection.py:85\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 85\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\urllib3\\util\\connection.py:73\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     72\u001b[0m     sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[1;32m---> 73\u001b[0m \u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "\u001b[1;31mConnectionAbortedError\u001b[0m: [WinError 10053] An established connection was aborted by the software in your host machine",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\urllib3\\connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    788\u001b[0m     conn,\n\u001b[0;32m    789\u001b[0m     method,\n\u001b[0;32m    790\u001b[0m     url,\n\u001b[0;32m    791\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[0;32m    792\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    793\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    794\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    795\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[0;32m    796\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[0;32m    797\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    798\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    799\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[0;32m    800\u001b[0m )\n\u001b[0;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\urllib3\\connectionpool.py:488\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    487\u001b[0m         new_e \u001b[38;5;241m=\u001b[39m _wrap_proxy_error(new_e, conn\u001b[38;5;241m.\u001b[39mproxy\u001b[38;5;241m.\u001b[39mscheme)\n\u001b[1;32m--> 488\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m new_e\n\u001b[0;32m    490\u001b[0m \u001b[38;5;66;03m# conn.request() calls http.client.*.request, not the method in\u001b[39;00m\n\u001b[0;32m    491\u001b[0m \u001b[38;5;66;03m# urllib3.request. It also calls makefile (recv) on the socket.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\urllib3\\connectionpool.py:464\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 464\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\urllib3\\connectionpool.py:1093\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   1092\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[1;32m-> 1093\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1095\u001b[0m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\urllib3\\connection.py:704\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    703\u001b[0m sock: socket\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m|\u001b[39m ssl\u001b[38;5;241m.\u001b[39mSSLSocket\n\u001b[1;32m--> 704\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m sock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    705\u001b[0m server_hostname: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\urllib3\\connection.py:213\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 213\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NewConnectionError(\n\u001b[0;32m    214\u001b[0m         \u001b[38;5;28mself\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to establish a new connection: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    215\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    217\u001b[0m sys\u001b[38;5;241m.\u001b[39maudit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp.client.connect\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mport)\n",
      "\u001b[1;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPSConnection object at 0x0000018F71DEBA30>: Failed to establish a new connection: [WinError 10053] An established connection was aborted by the software in your host machine",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\requests\\adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\urllib3\\connectionpool.py:841\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    839\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[1;32m--> 841\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    842\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    843\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    844\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\urllib3\\util\\retry.py:519\u001b[0m, in \u001b[0;36mRetry.increment\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    518\u001b[0m     reason \u001b[38;5;241m=\u001b[39m error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause)\n\u001b[1;32m--> 519\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mreason\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    521\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
      "\u001b[1;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='en.wikipedia.org', port=443): Max retries exceeded with url: /w/api.php?meta=siteinfo%7Cuserinfo%7Cuserinfo&siprop=general%7Cnamespaces&uiprop=groups%7Crights%7Cblockinfo%7Chasmsg&continue=&action=query&format=json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x0000018F71DEBA30>: Failed to establish a new connection: [WinError 10053] An established connection was aborted by the software in your host machine'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m wikipedia_sections \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m title \u001b[38;5;129;01min\u001b[39;00m titles:\n\u001b[1;32m----> 3\u001b[0m     wikipedia_sections\u001b[38;5;241m.\u001b[39mextend(\u001b[43mall_subsections_from_title\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(wikipedia_sections)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m sections in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(titles)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m pages.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[7], line 11\u001b[0m, in \u001b[0;36mall_subsections_from_title\u001b[1;34m(title, sections_to_ignore, site_name)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mall_subsections_from_title\u001b[39m(\n\u001b[0;32m      2\u001b[0m     title: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m      3\u001b[0m     sections_to_ignore: \u001b[38;5;28mset\u001b[39m[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m SECTIONS_TO_IGNORE,\n\u001b[0;32m      4\u001b[0m     site_name: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m WIKI_SITE,\n\u001b[0;32m      5\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m], \u001b[38;5;28mstr\u001b[39m]]:\n\u001b[0;32m      6\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"From a Wikipedia page title, return a flattened list of all nested subsections.\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03m    Each subsection is a tuple, where:\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124;03m        - the first element is a list of parent subtitles, starting with the page title\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03m        - the second element is the text of the subsection (but not any children)\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m     site \u001b[38;5;241m=\u001b[39m \u001b[43mmwclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSite\u001b[49m\u001b[43m(\u001b[49m\u001b[43msite_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     page \u001b[38;5;241m=\u001b[39m site\u001b[38;5;241m.\u001b[39mpages[title]\n\u001b[0;32m     13\u001b[0m     text \u001b[38;5;241m=\u001b[39m page\u001b[38;5;241m.\u001b[39mtext()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\mwclient\\client.py:186\u001b[0m, in \u001b[0;36mSite.__init__\u001b[1;34m(self, host, path, ext, pool, retry_timeout, max_retries, wait_callback, clients_useragent, max_lag, compress, force_login, do_init, httpauth, connection_options, consumer_token, consumer_secret, access_token, access_secret, client_certificate, custom_headers, scheme, reqs)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_init:\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 186\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msite_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mAPIError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    188\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmwoauth-invalid-authorization\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\mwclient\\client.py:209\u001b[0m, in \u001b[0;36mSite.site_init\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokens \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 209\u001b[0m meta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mquery\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msiteinfo|userinfo\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    210\u001b[0m \u001b[43m                \u001b[49m\u001b[43msiprop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgeneral|namespaces\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muiprop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgroups|rights\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    211\u001b[0m \u001b[43m                \u001b[49m\u001b[43mretry_on_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;66;03m# Extract site info\u001b[39;00m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msite \u001b[38;5;241m=\u001b[39m meta[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgeneral\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\mwclient\\client.py:300\u001b[0m, in \u001b[0;36mSite.get\u001b[1;34m(self, action, *args, **kwargs)\u001b[0m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, action, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    289\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Perform a generic API call using GET.\u001b[39;00m\n\u001b[0;32m    290\u001b[0m \n\u001b[0;32m    291\u001b[0m \u001b[38;5;124;03m    This is just a shorthand for calling api() with http_method='GET'.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;124;03m        The raw response from the API call, as a dictionary.\u001b[39;00m\n\u001b[0;32m    299\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 300\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi(action, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGET\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\mwclient\\client.py:358\u001b[0m, in \u001b[0;36mSite.api\u001b[1;34m(self, action, http_method, *args, **kwargs)\u001b[0m\n\u001b[0;32m    355\u001b[0m sleeper \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msleepers\u001b[38;5;241m.\u001b[39mmake()\n\u001b[0;32m    357\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 358\u001b[0m     info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_api(action, http_method, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    359\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m info:\n\u001b[0;32m    360\u001b[0m         info \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\mwclient\\client.py:569\u001b[0m, in \u001b[0;36mSite.raw_api\u001b[1;34m(self, action, http_method, retry_on_error, *args, **kwargs)\u001b[0m\n\u001b[0;32m    567\u001b[0m kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mformat\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjson\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    568\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_query_string(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 569\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mapi\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretry_on_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry_on_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    570\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mhttp_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhttp_method\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    572\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    573\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m json\u001b[38;5;241m.\u001b[39mloads(res, object_pairs_hook\u001b[38;5;241m=\u001b[39mOrderedDict)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\mwclient\\client.py:497\u001b[0m, in \u001b[0;36mSite.raw_call\u001b[1;34m(self, script, data, files, retry_on_error, http_method)\u001b[0m\n\u001b[0;32m    494\u001b[0m     args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data\n\u001b[0;32m    496\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 497\u001b[0m     stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnection\u001b[38;5;241m.\u001b[39mrequest(http_method, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m    498\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx-database-lag\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    499\u001b[0m         wait_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(stream\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mretry-after\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\requests\\adapters.py:700\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    696\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, _SSLError):\n\u001b[0;32m    697\u001b[0m         \u001b[38;5;66;03m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[0;32m    698\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m--> 700\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClosedPoolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    703\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "\u001b[1;31mConnectionError\u001b[0m: HTTPSConnectionPool(host='en.wikipedia.org', port=443): Max retries exceeded with url: /w/api.php?meta=siteinfo%7Cuserinfo%7Cuserinfo&siprop=general%7Cnamespaces&uiprop=groups%7Crights%7Cblockinfo%7Chasmsg&continue=&action=query&format=json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x0000018F71DEBA30>: Failed to establish a new connection: [WinError 10053] An established connection was aborted by the software in your host machine'))"
     ]
    }
   ],
   "source": [
    "wikipedia_sections = []\n",
    "for title in titles:\n",
    "    wikipedia_sections.extend(all_subsections_from_title(title))\n",
    "print(f\"Found {len(wikipedia_sections)} sections in {len(titles)} pages.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered out 0 sections, leaving 1706 sections.\n",
      "['Almaty bid for the 2022 Winter Olympics']\n",
      "{{Short description|Unsuccessful bid by Almaty, Kazakhstan}}\n",
      "{{Olympic bid|20...\n",
      "\n",
      "['Almaty bid for the 2022 Winter Olympics', '==History==']\n",
      "[[File:Medeobanen1.jpg|thumb|230px|right|[[Medeo]], a probable location near ...\n",
      "\n",
      "['Almaty bid for the 2022 Winter Olympics', '==History==', '===Previous bids===']\n",
      "{{main|Almaty bid for the 2014 Winter Olympics}}\n",
      "Almaty bid to host the [[201...\n",
      "\n",
      "['Almaty bid for the 2022 Winter Olympics', '==Venues==']\n",
      "The venue plan comprises:...\n",
      "\n",
      "['Almaty bid for the 2022 Winter Olympics', '==Venues==', '=== City venues ===', '==== Olympic City ====']\n",
      "*Almaty Olympic Ice Arena - Figure skating, short track\n",
      "*Non-competitive venu...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# clean text\n",
    "def clean_section(section: tuple[list[str], str]) -> tuple[list[str], str]:\n",
    "    \"\"\"\n",
    "    Return a cleaned up section with:\n",
    "        - <ref>xyz</ref> patterns removed\n",
    "        - leading/trailing whitespace removed\n",
    "    \"\"\"\n",
    "    titles, text = section\n",
    "    text = re.sub(r\"<ref.*?</ref>\", \"\", text)\n",
    "    text = text.strip()\n",
    "    return (titles, text)\n",
    "\n",
    "\n",
    "wikipedia_sections = [clean_section(ws) for ws in wikipedia_sections]\n",
    "\n",
    "# Filter out short/blank sections\n",
    "def keep_section(section: tuple[list[str], str]) -> bool:\n",
    "    _, text = section\n",
    "    return len(text) >= 16\n",
    "\n",
    "\n",
    "original_num_sections = len(wikipedia_sections)\n",
    "wikipedia_sections = [ws for ws in wikipedia_sections if keep_section(ws)]\n",
    "print(f\"Filtered out {original_num_sections-len(wikipedia_sections)} sections, leaving {len(wikipedia_sections)} sections.\")\n",
    "\n",
    "# Display example data\n",
    "for ws in wikipedia_sections[:5]:\n",
    "    print(ws[0])\n",
    "    print(ws[1][:77] + \"...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v6Wasvbi26gM"
   },
   "source": [
    "### Handle Text Length (Tokens)\n",
    "\n",
    "Embedding models perform best when input text stays within reasonable token limits.\n",
    "To achieve this, we measure token counts using a model-compatible tokenizer and\n",
    "split long sections into balanced chunks while preserving natural text boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Us1DUzXQ26Bq",
    "outputId": "1cfee394-935d-4480-be08-3fd6e429563a"
   },
   "outputs": [],
   "source": [
    "GPT_MODEL = \"gpt-4o-mini\"\n",
    "# Used only to select a compatible tokenizer via tiktoken.\n",
    "# No OpenAI model inference is performed.\n",
    "\n",
    "def num_tokens(text: str, model: str = GPT_MODEL) -> int:\n",
    "    \"\"\"\n",
    "    Count the number of tokens in a text string using a model-specific tokenizer.\n",
    "\n",
    "    This function is used to ensure text chunks remain within reasonable\n",
    "    token limits for embedding generation.\n",
    "\n",
    "    Args:\n",
    "        text: Input text string.\n",
    "        model: Model name used only to select the tokenizer.\n",
    "\n",
    "    Returns:\n",
    "        Number of tokens in the input text.\n",
    "    \"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    return len(encoding.encode(text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function counts the number of tokens (units of text) in a given string based on a specific tokenizer model (in this case, gpt-4o-mini). \n",
    "\n",
    "Here's what each part does:\n",
    "\n",
    "1. Input: A text string.\n",
    "2. Output: The number of tokens in the string.\n",
    "How: It uses a tokenizer specific to the given model (like how many words or chunks the model recognizes) to \"encode\" the text and count its tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def halved_by_delimiter(string: str, delimiter: str = \"\\n\") -> list[str, str]:\n",
    "    chunks = string.split(delimiter)\n",
    "    if len(chunks) == 1:\n",
    "        return [string, \"\"]\n",
    "    total_tokens = num_tokens(string)\n",
    "    halfway = total_tokens // 2\n",
    "    best_diff = halfway\n",
    "    for i, _ in enumerate(chunks):\n",
    "        left = delimiter.join(chunks[: i + 1])\n",
    "        left_tokens = num_tokens(left)\n",
    "        diff = abs(halfway - left_tokens)\n",
    "        if diff >= best_diff:\n",
    "            break\n",
    "        best_diff = diff\n",
    "    left = delimiter.join(chunks[:i])\n",
    "    right = delimiter.join(chunks[i:])\n",
    "    return [left, right]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function splits a large string into two parts at a logical breakpoint (like a sentence or paragraph).\n",
    "\n",
    "Input:\n",
    "1. string: A large text string.\n",
    "2. delimiter: The point where we want to split the string (default is a new line \\n).\n",
    "3. Output: Two parts of the string, left and right.\n",
    "\n",
    "How: It tries to find the point where the string should be split into two halves based on token count. It looks for the closest match to half the total number of tokens, and then splits the string into two logical parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncated_string(\n",
    "    string: str,\n",
    "    model: str,\n",
    "    max_tokens: int,\n",
    "    print_warning: bool = True\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Truncate a text string to a maximum number of tokens using a tokenizer.\n",
    "\n",
    "    The function encodes the input text into tokens, trims it to the specified\n",
    "    token limit if necessary, and decodes it back into a string. This ensures\n",
    "    compatibility with embedding and generation model token constraints.\n",
    "\n",
    "    Args:\n",
    "        string: Input text to truncate.\n",
    "        model: Model name used to select the tokenizer.\n",
    "        max_tokens: Maximum number of tokens allowed.\n",
    "        print_warning: Whether to print a warning if truncation occurs.\n",
    "\n",
    "    Returns:\n",
    "        The truncated text string.\n",
    "    \"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    encoded_string = encoding.encode(string)\n",
    "    truncated = encoding.decode(encoded_string[:max_tokens])\n",
    "\n",
    "    if print_warning and len(encoded_string) > max_tokens:\n",
    "        print(\n",
    "            f\"Warning: Truncated string from \"\n",
    "            f\"{len(encoded_string)} tokens to {max_tokens} tokens.\"\n",
    "        )\n",
    "\n",
    "    return truncated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function ensures that text does not exceed a specified token limit by safely truncating it using a tokenizer-compatible encoding and decoding process.\n",
    "\n",
    "Input:\n",
    "\n",
    "1. string: The text to be truncated.\n",
    "2. model: The tokenizer model to use.\n",
    "3. max_tokens: The maximum number of tokens allowed.\n",
    "4. print_warning: If the string is shortened, a warning will be printed.\n",
    "\n",
    "Output: The truncated string (cut down to the allowed number of tokens).\n",
    "How: It encodes the string into tokens, and if the number of tokens exceeds the limit, it trims the string and prints a warning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1706 Wikipedia sections split into 2006 strings.\n"
     ]
    }
   ],
   "source": [
    "def split_strings_from_subsection(subsection: tuple[list[str], str], max_tokens: int = 1000, model: str = GPT_MODEL, max_recursion: int = 5) -> list[str]:\n",
    "    titles, text = subsection\n",
    "    string = \"\\n\\n\".join(titles + [text])\n",
    "    if num_tokens(string) <= max_tokens:\n",
    "        return [string]\n",
    "    elif max_recursion == 0:\n",
    "        return [truncated_string(string, model=model, max_tokens=max_tokens)]\n",
    "    for delimiter in [\"\\n\\n\", \"\\n\", \". \"]:\n",
    "        left, right = halved_by_delimiter(text, delimiter=delimiter)\n",
    "        if left == \"\" or right == \"\":\n",
    "            continue\n",
    "        results = []\n",
    "        for half in [left, right]:\n",
    "            half_subsection = (titles, half)\n",
    "            half_strings = split_strings_from_subsection(\n",
    "                half_subsection,\n",
    "                max_tokens=max_tokens,\n",
    "                model=model,\n",
    "                max_recursion=max_recursion - 1,\n",
    "            )\n",
    "            results.extend(half_strings)\n",
    "        return results\n",
    "    return [truncated_string(string, model=model, max_tokens=max_tokens)]\n",
    "\n",
    "# Split sections into chunks\n",
    "MAX_TOKENS = 1600\n",
    "wikipedia_strings = []\n",
    "for section in wikipedia_sections:\n",
    "    wikipedia_strings.extend(split_strings_from_subsection(section, max_tokens=MAX_TOKENS))\n",
    "\n",
    "print(f\"{len(wikipedia_sections)} Wikipedia sections split into {len(wikipedia_strings)} strings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function breaks down a large section of text into smaller pieces that are below a token limit.\n",
    "\n",
    "Input:\n",
    "1. subsection: A tuple containing a list of titles (headers) and the main text.\n",
    "2. max_tokens: The maximum number of tokens each part can have.\n",
    "3. model: The tokenizer model.\n",
    "4. max_recursion: How many times the function can call itself to keep splitting the text if it's too big.\n",
    "\n",
    "Output: A list of smaller text strings, each below the token limit.\n",
    "\n",
    "How: It tries to break the text into smaller parts by splitting it at logical places (like paragraph breaks or sentence breaks) and keeps doing so recursively until the chunks are small enough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g_bTrfJk3Fto"
   },
   "source": [
    "### **Generate Embeddings**\n",
    "**Prepare Text for Embedding**\n",
    "\n",
    "Once the Wikipedia sections have been cleaned and split into token-safe chunks, we convert each chunk into a numerical embedding. These embeddings enable semantic similarity search and retrieval in downstream RAG workflows.\n",
    "\n",
    "To improve efficiency and reliability, embeddings are generated in batches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nRUXNpud3PsF",
    "outputId": "4615aa0b-3c4f-449e-df61-128ecb5c95cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1706 Wikipedia sections split into 2006 strings.\n",
      "Processing batch 0 to 99\n",
      "HTTP error occurred: 403 Client Error: Forbidden for url: https://api.nugen.in/inference/embeddings\n",
      "Response content: b'{\"detail\":\"Could not validate credentials. Reason: API key not found\"}'\n",
      "Processing batch 100 to 199\n",
      "HTTP error occurred: 403 Client Error: Forbidden for url: https://api.nugen.in/inference/embeddings\n",
      "Response content: b'{\"detail\":\"Could not validate credentials. Reason: API key not found\"}'\n",
      "Processing batch 200 to 299\n",
      "HTTP error occurred: 403 Client Error: Forbidden for url: https://api.nugen.in/inference/embeddings\n",
      "Response content: b'{\"detail\":\"Could not validate credentials. Reason: API key not found\"}'\n",
      "Processing batch 300 to 399\n",
      "HTTP error occurred: 403 Client Error: Forbidden for url: https://api.nugen.in/inference/embeddings\n",
      "Response content: b'{\"detail\":\"Could not validate credentials. Reason: API key not found\"}'\n",
      "Processing batch 400 to 499\n",
      "HTTP error occurred: 403 Client Error: Forbidden for url: https://api.nugen.in/inference/embeddings\n",
      "Response content: b'{\"detail\":\"Could not validate credentials. Reason: API key not found\"}'\n",
      "Processing batch 500 to 599\n",
      "HTTP error occurred: 403 Client Error: Forbidden for url: https://api.nugen.in/inference/embeddings\n",
      "Response content: b'{\"detail\":\"Could not validate credentials. Reason: API key not found\"}'\n",
      "Processing batch 600 to 699\n"
     ]
    }
   ],
   "source": [
    "# Split sections into chunks\n",
    "MAX_TOKENS = 1600\n",
    "wikipedia_strings = []\n",
    "for section in wikipedia_sections:\n",
    "    wikipedia_strings.extend(split_strings_from_subsection(section, max_tokens=MAX_TOKENS))\n",
    "\n",
    "print(f\"{len(wikipedia_sections)} Wikipedia sections split into {len(wikipedia_strings)} strings.\")\n",
    "\n",
    "# Fetch embeddings from Nugen API\n",
    "BATCH_SIZE = 100\n",
    "EMBEDDING_MODEL = \"nugen-flash-embed\"\n",
    "embeddings = []\n",
    "\n",
    "for batch_start in range(0, len(wikipedia_strings), BATCH_SIZE):\n",
    "    batch_end = batch_start + BATCH_SIZE\n",
    "    batch = wikipedia_strings[batch_start:batch_end]\n",
    "    print(f\"Processing batch {batch_start} to {batch_end-1}\")\n",
    "\n",
    "    payload = {\n",
    "        \"input\": batch,\n",
    "        \"model\": EMBEDDING_MODEL\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(url_api_server, json=payload, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        batch_embeddings = [e['embedding'] for e in data['data']]\n",
    "        embeddings.extend(batch_embeddings)\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(f\"HTTP error occurred: {e}\")\n",
    "        print(f\"Response content: {response.content}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mXpyARru3R-u"
   },
   "source": [
    "### **Save the Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D-QS2qxy3cEH",
    "outputId": "da2dfe5e-b777-432a-eecc-8344fec85c1c"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Save the embeddings\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mwikipedia_strings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43membedding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m SAVE_PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwinter_olympics_2022.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      4\u001b[0m df\u001b[38;5;241m.\u001b[39mto_csv(SAVE_PATH, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\frame.py:664\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    658\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[0;32m    659\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[0;32m    660\u001b[0m     )\n\u001b[0;32m    662\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    663\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[1;32m--> 664\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    665\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[0;32m    666\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmrecords\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmrecords\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\internals\\construction.py:493\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    489\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    490\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[0;32m    491\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[1;32m--> 493\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\internals\\construction.py:118\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 118\u001b[0m         index \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    120\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\internals\\construction.py:666\u001b[0m, in \u001b[0;36m_extract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    664\u001b[0m lengths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(raw_lengths))\n\u001b[0;32m    665\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lengths) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 666\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll arrays must be of the same length\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    668\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m have_dicts:\n\u001b[0;32m    669\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    670\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    671\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "# Save the embeddings\n",
    "df = pd.DataFrame({\"text\": wikipedia_strings, \"embedding\": embeddings})\n",
    "SAVE_PATH = \"winter_olympics_2022.csv\"\n",
    "df.to_csv(SAVE_PATH, index=False)\n",
    "print(f\"Embeddings saved to {SAVE_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
